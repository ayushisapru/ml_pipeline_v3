## FLTS AI Agent Guide (Concise, Actionable)

Keep this <50 lines. Focus: dataflow, idempotency, promotion logic, extension hooks, and gotchas that break the pipeline.

1. End‑to‑End Flow (claim‑check pattern): Preprocess -> Train (GRU,LSTM,PROPHET + deep variants: TETS, TCN, EncoderLSTM) -> Eval/Promotion -> Inference. Kafka messages are tiny JSON pointing to MinIO objects accessed ONLY via `fastapi-app` (`/upload/{bucket}/{obj}`, `/download/{bucket}/{obj}`). MLflow (Postgres backend + MinIO artifacts) is the system of record.
2. Topics (default names): preprocess PRODUCER_TOPIC_0=`training-data`, PRODUCER_TOPIC_1=`inference-data`; trainers CONSUMER_TOPIC=`training-data` -> publish `model-training`; eval consumes `model-training` -> publishes `model-selected`; inference consumes `inference-data`, `model-training` (optional fast path), and `model-selected` (promotion events) -> may publish performance metrics. DLQ pattern: `DLQ-<base_topic>` or explicit env (e.g. `DLQ_MODEL_SELECTED`).
3. Preprocess (`preprocess_container/main.py`): Canonical config = `build_active_config()` + `_data` (filenames + sampling) + `EXTRA_HASH_SALT`; deterministic JSON (sorted, compact) -> SHA256 `config_hash`. If `<train_base>.meta.json` has same hash and `FORCE_REPROCESS!=1`, skip heavy work and re-emit claim checks. ALWAYS embed Parquet metadata keys: `preprocess_config`, `config_hash`. Time features must output stable `{min_of_day,day_of_week,day_of_year}_{sin,cos}` or inference will reject. Sampling envs (`SAMPLE_*`) affect hash—never add new semantic env vars without adding them to canonical config.
4. Claim JSON Examples: Preprocess -> `{bucket:"processed-data",object:"processed_data.parquet",config_hash:"<sha256>",identifier:"id"}`; Train SUCCESS (only after artifacts) -> `{operation:"Trained: GRU",status:"SUCCESS",config_hash:"<sha256>",run_id:"<mlflow_run>"}`; Promotion -> `{model_uri:"runs:/<run_id>/<MODEL_TYPE>",config_hash:"<sha256>",...}`.
5. Training (`train_container/main.py`): Reads Parquet via gateway; `_extract_meta` pulls embedded config. Target enforcement: if no `value`, clone first numeric column to `value` then HARD DROP original (prevents feature inflation). Windowing precedes feature counting; final tensor last dim = `NUM_FEATURES` used for model init. Deduplicate configs: skip if `(MODEL_TYPE, config_hash)` already trained and `SKIP_DUPLICATE_CONFIGS=1` (bounded by `DUP_CACHE_MAX`). Artifacts: model under folder named exactly `MODEL_TYPE`; scaler at `scaler/scaler.pkl`. NEVER emit SUCCESS before all artifacts exist. Increment `VERSION` on logic changes.
6. Adding a Model Type: Add branch in `_build_model` + env vars; ensure artifact folder name == new `MODEL_TYPE`; add service block in `docker-compose.yaml` (unique `CONSUMER_GROUP_ID`); include type in `EXPECTED_MODEL_TYPES` for promotion; update every `_ensure_buckets()` (train/eval/inference) if new buckets required.
7. Evaluation (`eval_container/main.py`): Waits until ALL `EXPECTED_MODEL_TYPES` (default `GRU,LSTM,PROPHET`) emitted SUCCESS for same `config_hash`. Enumerates ALL experiments (heterogeneous family separation) and retries search (`PROMOTION_SEARCH_RETRIES`, `PROMOTION_SEARCH_DELAY_SEC`) to mitigate MLflow race. Score = 0.5*rmse + 0.3*mae + 0.2*mse (lower wins; tie -> newest start_time). Writes history: `model-promotion/<identifier|global>/<config_hash>/promotion-<ts>.json` + pointer `current.json` then publishes promotion event.
8. Inference (`inference_container/inferencer.py`): Model load tries `runs:/<run_id>/<run_name>` then fallback `runs:/<run_id>/model`. Framework detection from `params.model_type` or heuristics (pytorch vs prophet vs statsforecast). Requires rows >= `input_seq_len + output_seq_len`. Uses time feature sin/cos column names verbatim. Duplicate prediction suppression via `(run_id, prediction_hash)` set. Concurrency/env controls: `WAIT_FOR_MODEL`, `MODEL_WAIT_TIMEOUT`, `INFERENCE_PREWARM`, `RUN_INFERENCE_ON_TRAIN_SUCCESS`, `QUEUE_WORKERS`, `QUEUE_MAXSIZE`, `INFERENCE_TIMEOUT`, `DISABLE_STARTUP_INFERENCE`—preserve semantics if extending.
9. Logging Conventions: Structured one-line JSON with `service` + `event` key minimum. Key high-signal events: `skip_idempotent`, `target_fallback`, `feature_count_mismatch`, `promotion_waiting_for_models`, `promotion_runs_search`, `promotion_scoreboard`, `predict_inference_start`. Do NOT rename existing keys—dashboards rely on them.
10. Hash / Lineage Hygiene: Include every env that changes preprocessing semantics in canonical config. Use `EXTRA_HASH_SALT` to force a new lineage without logic changes; `FORCE_REPROCESS=1` to bypass idempotency cache. Stale hash = silent reuse issues (downstream sees old data but new expectations).
11. Local Dev Loop: Fast subset: tweak `SAMPLE_TRAIN_ROWS|SAMPLE_TEST_ROWS` + `EXTRA_HASH_SALT`, then `docker compose up --build preprocess train_gru train_lstm nonml_prophet eval inference`. To add another deep model: copy a `train_*` block, set new `MODEL_TYPE`, adjust hyperparams + `CONSUMER_GROUP_ID`.
12. Critical Pitfalls: (a) Emitting SUCCESS before artifacts => eval never sees folder. (b) Renaming/dropping sin/cos time features => inference shape rejection. (c) Forgetting to drop original target after synthesizing `value` => input_size mismatch. (d) Missing new config field in canonical JSON => cache hit when change expected. (e) Omit bucket in any `_ensure_buckets()` => MLflow artifact logging failure. (f) Artifact folder name mismatch with `MODEL_TYPE` => eval exclusion. (g) Promotion tie logic depends on start_time ordering—system clock skew can affect selection.
13. Version Markers: Bump `VERSION` (train), `EVAL_VERSION`, `INFER_VERSION` whenever functional logic changes so logs prove fresh deployment.

Update this file whenever cross‑service patterns, bucket names, env conventions, or model families change. Keep concrete and action-oriented.
