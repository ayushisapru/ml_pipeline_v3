## FLTS AI Agent Guide (Concise, Actionable)

Keep this under ~50 lines. Focus: dataflow, idempotency, promotion logic, extension hooks, and gotchas that break the pipeline.

1) End-to-end (claim-check): Preprocess -> Train (GRU,LSTM,PROPHET; deep: TETS, TCN, EncoderLSTM) -> Eval/Promotion -> Inference. Kafka messages are claim JSON; all object I/O goes through `fastapi-app` gateway (`/upload/{bucket}/{obj}`, `/download/{bucket}/{obj}`). MLflow (PG backend + MinIO artifacts) is source of truth.
2) Topics (defaults): preprocess publishes `training-data` and `inference-data`; trainers consume `training-data` -> publish `model-training`; eval consumes `model-training` -> publishes `model-selected`; inference consumes `inference-data`, `model-training` (fast-path), and `model-selected`. DLQ: `DLQ-<base>` or explicit env (e.g., `DLQ_MODEL_SELECTED`).
3) Preprocess (`preprocess_container/main.py`): Build canonical config = `build_active_config()` + `_data` (filenames+sampling) + `EXTRA_HASH_SALT`; JSON (sorted, compact) -> SHA256 `config_hash`. If `<train_base>.meta.json` has same hash and `FORCE_REPROCESS!=1`, skip and re-emit claims. Always embed Parquet metadata keys: `preprocess_config`, `config_hash`. Time features must be `{min_of_day,day_of_week,day_of_year}_{sin,cos}`. Sampling envs affect the hash.
4) Claim JSON examples: Preprocess -> `{bucket:"processed-data",object:"processed_data.parquet",config_hash:"<sha256>",identifier:"id"}`; Train SUCCESS (after artifacts) -> `{operation:"Trained: GRU",status:"SUCCESS",config_hash:"<sha256>",run_id:"<mlflow_run>"}`; Promotion -> `{model_uri:"runs:/<run_id>/<MODEL_TYPE>",config_hash:"<sha256>",...}`.
5) Training (`train_container/main.py`): Reads Parquet via gateway; `_extract_meta` pulls config. If no `value`, clone first numeric col to `value` then HARD DROP original (prevents feature inflation). Windowing precedes feature counting; infer `NUM_FEATURES` from tensors. Deduplicate: skip if `(MODEL_TYPE,config_hash)` seen and `SKIP_DUPLICATE_CONFIGS=1` (bounded by `DUP_CACHE_MAX`). Artifacts: model under folder `MODEL_TYPE`; scaler under `scaler/*.pkl`. Publish SUCCESS only after artifacts. Bump `VERSION` on logic changes. Tip: set explicit `CONSUMER_GROUP_ID` to avoid dev offset collisions (default randomizes).
6) Eval/Promotion (`eval_container/main.py`): Wait until ALL `EXPECTED_MODEL_TYPES` (default `GRU,LSTM,PROPHET`) SUCCESS for same `config_hash`. Enumerates ALL experiments; retries search (`PROMOTION_SEARCH_RETRIES`, `PROMOTION_SEARCH_DELAY_SEC`) to avoid MLflow races. Score = 0.5·rmse + 0.3·mae + 0.2·mse (lower wins; tie -> newest start_time). Writes history `model-promotion/<identifier|global>/<config_hash>/promotion-<ts>.json` and pointers: root `current.json` + legacy `global/current.json` + `<identifier>/current.json`; then publishes `model-selected`.
7) Inference (`inference_container/inferencer.py`, `main.py`): Loads via `runs:/<run_id>/<run_name>` then fallback `runs:/<run_id>/model`. Detects framework from `params.model_type` or heuristics (pytorch/prophet/statsforecast). Requires rows >= `input_seq_len + output_seq_len`. Uses time-feature names verbatim. Suppresses duplicate predictions via `(run_id, prediction_hash)`. Startup auto-load: pointer lookup order `current.json` (root) → `global/current.json` → `<identifier>/current.json`. Concurrency/env gates: `WAIT_FOR_MODEL`, `MODEL_WAIT_TIMEOUT`, `INFERENCE_PREWARM`, `RUN_INFERENCE_ON_TRAIN_SUCCESS`, `QUEUE_WORKERS`, `QUEUE_MAXSIZE`, `INFERENCE_TIMEOUT`, `DISABLE_STARTUP_INFERENCE`.
8) Buckets: ensure via `_ensure_buckets()` in train/eval/inference. Key buckets: `processed-data`, `mlflow`, `model-promotion`, `inference-logs`. Gotcha: trainer default `INFERENCE_LOG_BUCKET` may be `inference-txt-logs` while inference defaults to `inference-logs`—set env to keep consistent.
9) Logging conventions (one-line JSON, stable keys): `skip_idempotent`, `target_fallback`, `feature_count_mismatch`, `promotion_waiting_for_models`, `promotion_runs_search`, `promotion_scoreboard`, `promotion_artifacts_ok`, `predict_inference_start`, `promotion_model_loaded_startup`. Do not rename—dashboards depend on them.
10) Hash/Lineage hygiene: include every semantic env in canonical config. Use `EXTRA_HASH_SALT` for new lineage; `FORCE_REPROCESS=1` to bypass cache. Stale hash => silent reuse.
11) Local dev loop: use `SAMPLE_TRAIN_ROWS|SAMPLE_TEST_ROWS` + `EXTRA_HASH_SALT` then `docker compose up --build preprocess train_gru train_lstm nonml_prophet eval inference`. Optional load test: `locust` service (`locust/locustfile.py`); web UI at `http://localhost:8089`.
12) Add a model type: branch in `_build_model` + envs; ensure artifact folder name == `MODEL_TYPE`; add a `train_*` compose service (unique `CONSUMER_GROUP_ID`); include in `EXPECTED_MODEL_TYPES`; update all `_ensure_buckets()` if new buckets required.
13) Critical pitfalls: (a) Publish SUCCESS before artifacts => eval misses model. (b) Rename/drop sin/cos time features => inference shape errors. (c) Forget to drop original target after synthesizing `value` => input_size mismatch. (d) Miss new config field in canonical JSON => stale hash reuse. (e) Missing bucket in any `_ensure_buckets()` => artifact logging failures. (f) Artifact folder name ≠ `MODEL_TYPE` => eval exclusion. (g) Clock skew affects tie-breaks. (h) Inconsistent `INFERENCE_LOG_BUCKET` names scatter logs.
14) Version markers: bump `VERSION` (train), `EVAL_VERSION`, `INFER_VERSION` with functional changes so logs prove fresh deployment.

Update this file when cross‑service patterns, bucket names, env conventions, or model families change. Keep concrete and action-oriented.
