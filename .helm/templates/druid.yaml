---
# Druid Postgres Service
apiVersion: v1
kind: Service
metadata:
  name: druid-postgres
  labels:
    app.kubernetes.io/name: druid-postgres
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  type: ClusterIP
  ports:
    - port: 5432
      targetPort: 5432
      protocol: TCP
      name: postgres
  selector:
    app.kubernetes.io/name: druid-postgres
    app.kubernetes.io/instance: {{ .Release.Name }}

---
# Druid Postgres Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: druid-postgres
  labels:
    app.kubernetes.io/name: druid-postgres
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: druid-postgres
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: druid-postgres
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: postgres
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "postgres:latest"
          imagePullPolicy: {{ .Values.global.pullPolicy }}
          ports:
            - name: postgres
              containerPort: 5432
              protocol: TCP
          env:
            - name: POSTGRES_PASSWORD
              value: "FoolishPassword"
            - name: POSTGRES_USER
              value: "druid"
            - name: POSTGRES_DB
              value: "druid"
          resources:
            requests:
              memory: "{{ .Values.druid.postgres.resources.requests.memory }}"
              cpu: "{{ .Values.druid.postgres.resources.requests.cpu }}"
            limits:
              memory: "{{ .Values.druid.postgres.resources.limits.memory }}"
              cpu: "{{ .Values.druid.postgres.resources.limits.cpu }}"
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - druid
                - -d
                - druid
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - druid
                - -d
                - druid
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          resources:
            {{- toYaml .Values.resources.postgres | nindent 12 }}
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: druid-postgres-pvc

---
# Zookeeper Service
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  labels:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  type: ClusterIP
  ports:
    - port: 2181
      targetPort: 2181
      protocol: TCP
      name: zookeeper
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: {{ .Release.Name }}

---
# Zookeeper Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
  labels:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: zookeeper
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: zookeeper
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "zookeeper:3.5.10"
          imagePullPolicy: {{ .Values.global.pullPolicy }}
          ports:
            - name: zookeeper
              containerPort: 2181
              protocol: TCP
          env:
            - name: ZOO_MY_ID
              value: "1"
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "250m"

---
# Druid Coordinator Service
apiVersion: v1
kind: Service
metadata:
  name: druid-coordinator
  labels:
    app.kubernetes.io/name: druid-coordinator
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  type: ClusterIP
  ports:
    - port: 8081
      targetPort: 8081
      protocol: TCP
      name: coordinator
  selector:
    app.kubernetes.io/name: druid-coordinator
    app.kubernetes.io/instance: {{ .Release.Name }}

---
# Druid Coordinator Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: druid-coordinator
  labels:
    app.kubernetes.io/name: druid-coordinator
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: druid-coordinator
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: druid-coordinator
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: wait-for-postgres
          image: postgres:13
          command:
            - sh
            - -c
            - |
              until pg_isready -h druid-postgres -p 5432 -U druid; do
                echo "Waiting for postgres..."
                sleep 2
              done
        - name: wait-for-zookeeper
          image: busybox:1.35
          command:
            - sh
            - -c
            - |
              until nc -z zookeeper 2181; do
                echo "Waiting for Zookeeper..."
                sleep 2
              done
      containers:
        - name: coordinator
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "apache/druid:34.0.0"
          imagePullPolicy: {{ .Values.global.pullPolicy }}
          args: ["coordinator"]
          ports:
            - name: coordinator
              containerPort: 8081
              protocol: TCP
          env:
            - name: DRUID_HOST
              value: "druid-coordinator"
          envFrom:
            - configMapRef:
                name: druid-config
          volumeMounts:
            - name: druid-shared
              mountPath: /opt/shared
            - name: coordinator-var
              mountPath: /opt/druid/var
          resources:
            requests:
              memory: "1.5Gi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "500m"
      volumes:
        - name: druid-shared
          persistentVolumeClaim:
            claimName: druid-shared-pvc
        - name: coordinator-var
          persistentVolumeClaim:
            claimName: druid-coordinator-pvc

---
# Druid Broker Service
apiVersion: v1
kind: Service
metadata:
  name: druid-broker
  labels:
    app.kubernetes.io/name: druid-broker
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  type: ClusterIP
  ports:
    - port: 8082
      targetPort: 8082
      protocol: TCP
      name: broker
  selector:
    app.kubernetes.io/name: druid-broker
    app.kubernetes.io/instance: {{ .Release.Name }}

---
# Druid Broker Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: druid-broker
  labels:
    app.kubernetes.io/name: druid-broker
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: druid-broker
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: druid-broker
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: wait-for-coordinator
          image: busybox:1.35
          command:
            - sh
            - -c
            - |
              until nc -z druid-coordinator 8081; do
                echo "Waiting for Coordinator..."
                sleep 2
              done
      containers:
        - name: broker
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "apache/druid:34.0.0"
          imagePullPolicy: {{ .Values.global.pullPolicy }}
          args: ["broker"]
          ports:
            - name: broker
              containerPort: 8082
              protocol: TCP
          env:
            - name: DRUID_HOST
              value: "druid-broker"
          envFrom:
            - configMapRef:
                name: druid-config
          volumeMounts:
            - name: broker-var
              mountPath: /opt/druid/var
          resources:
            requests:
              memory: "1.5Gi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "500m"
      volumes:
        - name: broker-var
          persistentVolumeClaim:
            claimName: druid-broker-pvc

{{- if .Values.druid.services.historical.enabled }}
---
# Druid Historical Service
apiVersion: v1
kind: Service
metadata:
  name: druid-historical
  labels:
    app.kubernetes.io/name: druid-historical
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  type: ClusterIP
  ports:
    - port: 8083
      targetPort: 8083
      protocol: TCP
      name: historical
  selector:
    app.kubernetes.io/name: druid-historical
    app.kubernetes.io/instance: {{ .Release.Name }}

---
# Druid Historical Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: druid-historical
  labels:
    app.kubernetes.io/name: druid-historical
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: druid-historical
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: druid-historical
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: wait-for-coordinator
          image: busybox:1.35
          command:
            - sh
            - -c
            - |
              until nc -z druid-coordinator 8081; do
                echo "Waiting for Coordinator..."
                sleep 2
              done
      containers:
        - name: historical
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "apache/druid:34.0.0"
          imagePullPolicy: {{ .Values.global.pullPolicy }}
          args: ["historical"]
          ports:
            - name: historical
              containerPort: 8083
              protocol: TCP
          env:
            - name: DRUID_HOST
              value: "druid-historical"
          envFrom:
            - configMapRef:
                name: druid-config
          volumeMounts:
            - name: druid-shared
              mountPath: /opt/shared
            - name: historical-var
              mountPath: /opt/druid/var
          resources:
            requests:
              memory: "1.5Gi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "500m"
      volumes:
        - name: druid-shared
          persistentVolumeClaim:
            claimName: druid-shared-pvc
        - name: historical-var
          persistentVolumeClaim:
            claimName: druid-historical-pvc
{{- end }}

---
# Druid MiddleManager Service
apiVersion: v1
kind: Service
metadata:
  name: druid-middlemanager
  labels:
    app.kubernetes.io/name: druid-middlemanager
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  type: ClusterIP
  ports:
    - port: 8091
      targetPort: 8091
      protocol: TCP
      name: middlemanager
    - port: 8100
      targetPort: 8100
      protocol: TCP
      name: peon-8100
    - port: 8101
      targetPort: 8101
      protocol: TCP
      name: peon-8101
    - port: 8102
      targetPort: 8102
      protocol: TCP
      name: peon-8102
    - port: 8103
      targetPort: 8103
      protocol: TCP
      name: peon-8103
    - port: 8104
      targetPort: 8104
      protocol: TCP
      name: peon-8104
    - port: 8105
      targetPort: 8105
      protocol: TCP
      name: peon-8105
  selector:
    app.kubernetes.io/name: druid-middlemanager
    app.kubernetes.io/instance: {{ .Release.Name }}

---
# Druid MiddleManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: druid-middlemanager
  labels:
    app.kubernetes.io/name: druid-middlemanager
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: druid-middlemanager
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: druid-middlemanager
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: wait-for-coordinator
          image: busybox:1.35
          command:
            - sh
            - -c
            - |
              until nc -z druid-coordinator 8081; do
                echo "Waiting for Coordinator..."
                sleep 2
              done
      containers:
        - name: middlemanager
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "apache/druid:34.0.0"
          imagePullPolicy: {{ .Values.global.pullPolicy }}
          args: ["middleManager"]
          ports:
            - name: middlemanager
              containerPort: 8091
              protocol: TCP
            - name: peon-8100
              containerPort: 8100
              protocol: TCP
            - name: peon-8101
              containerPort: 8101
              protocol: TCP
            - name: peon-8102
              containerPort: 8102
              protocol: TCP
            - name: peon-8103
              containerPort: 8103
              protocol: TCP
            - name: peon-8104
              containerPort: 8104
              protocol: TCP
            - name: peon-8105
              containerPort: 8105
              protocol: TCP
          env:
            - name: DRUID_HOST
              value: "druid-middlemanager"
          envFrom:
            - configMapRef:
                name: druid-config
          volumeMounts:
            - name: druid-shared
              mountPath: /opt/shared
            - name: middle-var
              mountPath: /opt/druid/var
          resources:
            requests:
              memory: "1.5Gi"
              cpu: "250m"
            limits:
              memory: "3Gi"
              cpu: "1000m"
      volumes:
        - name: druid-shared
          persistentVolumeClaim:
            claimName: druid-shared-pvc
        - name: middle-var
          persistentVolumeClaim:
            claimName: druid-middle-pvc

---
# Druid Router Service
apiVersion: v1
kind: Service
metadata:
  name: druid-router
  labels:
    app.kubernetes.io/name: druid-router
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  type: ClusterIP
  ports:
    - port: 8888
      targetPort: 8888
      protocol: TCP
      name: router
  selector:
    app.kubernetes.io/name: druid-router
    app.kubernetes.io/instance: {{ .Release.Name }}

---
# Druid Router Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: druid-router
  labels:
    app.kubernetes.io/name: druid-router
    app.kubernetes.io/instance: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: druid-router
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: druid-router
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: wait-for-coordinator
          image: busybox:1.35
          command:
            - sh
            - -c
            - |
              until nc -z druid-coordinator 8081; do
                echo "Waiting for Coordinator..."
                sleep 2
              done
      containers:
        - name: router
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "apache/druid:34.0.0"
          imagePullPolicy: {{ .Values.global.pullPolicy }}
          args: ["router"]
          ports:
            - name: router
              containerPort: 8888
              protocol: TCP
          env:
            - name: DRUID_HOST
              value: "druid-router"
          envFrom:
            - configMapRef:
                name: druid-config
          volumeMounts:
            - name: router-var
              mountPath: /opt/druid/var
          resources:
            requests:
              memory: "512Mi"
              cpu: "100m"
            limits:
              memory: "1Gi"
              cpu: "250m"
      volumes:
        - name: router-var
          persistentVolumeClaim:
            claimName: druid-router-pvc
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}