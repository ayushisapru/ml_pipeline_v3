"""FastAPI inference API with bounded async queue for overload protection.

Adds an in-process asyncio.Queue between request intake and model inference:
  - Bounded size (QUEUE_MAXSIZE) -> overflow returns 429 immediately.
  - Worker tasks (QUEUE_WORKERS) pull jobs sequentially / limited parallel.
  - Per-job inference timeout (INFERENCE_TIMEOUT) returns 504 if exceeded.
  - Preserves original /predict response schema & lazy model loading.
  - Provides /queue_stats for simple operational insight.
"""
from __future__ import annotations

from fastapi import FastAPI, HTTPException, Request, Body, Query, Response
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
import os, uuid, math, traceback
import threading
import asyncio, time
import pandas as pd

app = FastAPI(title="Inference Synchronous API")

# ---------------- Queue / Worker & Readiness Config (env-driven) -----------------
QUEUE_MAXSIZE = int(os.getenv("QUEUE_MAXSIZE", "40"))  # raised default for smoother concurrency
QUEUE_WORKERS = max(1, int(os.getenv("QUEUE_WORKERS", "1")))
INFERENCE_TIMEOUT = float(os.getenv("INFERENCE_TIMEOUT", "15"))
WAIT_FOR_MODEL = os.getenv("WAIT_FOR_MODEL", "0").lower() in {"1", "true", "yes"}
MODEL_WAIT_TIMEOUT = float(os.getenv("MODEL_WAIT_TIMEOUT", "60"))  # seconds
PREWARM_ENABLED = os.getenv("INFERENCE_PREWARM", "1").lower() in {"1", "true", "yes"}

_startup_epoch = time.time()
_startup_ready_ms: float | None = None


_inference_queue: asyncio.Queue | None = None
_workers_started = False

# Serialize underlying model.predict / perform_inference calls to avoid shared state races.
_MODEL_INFER_LOCK = threading.Lock()

queue_metrics = {
    "enqueued": 0,
    "rejected_full": 0,
    "rejected_busy": 0,
    "active": 0,
    "completed": 0,
    "timeouts": 0,
    "served_cached": 0,
    # wait time accounting
    "total_wait_ms": 0,
    "wait_samples": 0,
    # error metrics
    "error_500_total": 0,
    "last_error_type": None,
}

# (Removed rolling inference duration tracking in rollback)

# --------------- Fallback MLflow latest-model loader -----------------
def _fallback_load_latest_model(reason: str = "startup") -> bool:
    """Attempt to load the most recent usable MLflow run when no promotion manifest exists.

    Selection logic:
      1. Try runs (across all experiments) with tag promoted=true ordered by end_time DESC.
      2. Fallback: any FINISHED runs ordered by end_time DESC.
    A run is considered usable if it contains a model artifact folder named after its param 'model_type'.

    On success: sets inferencer.current_model and related metadata, returns True.
    On failure: returns False (logs structured events for observability).
    """
    try:  # noqa: C901 (keep logic linear & explicit)
        from main import inferencer as _inf, _enrich_loaded_model  # type: ignore
        import mlflow
        from mlflow.tracking import MlflowClient
        from mlflow import pyfunc
        client = MlflowClient()
        mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000"))
        # Enumerate experiments
        try:
            if hasattr(client, "search_experiments"):
                experiments = client.search_experiments()
            elif hasattr(client, "list_experiments"):
                experiments = client.list_experiments()
            else:
                experiments = []
        except Exception as ee:  # noqa: BLE001
            experiments = []
            print({"service": "inference", "event": "fallback_experiments_enum_fail", "error": str(ee)})
        exp_ids = [e.experiment_id for e in experiments]
        # Provide a minimal fallback if enumeration failed
        if not exp_ids:
            exp_ids = ["0"]  # 'Default' typical id
        print({"service": "inference", "event": "fallback_search_start", "reason": reason, "experiments": exp_ids})

        def _search(filter_string: str):
            try:
                return client.search_runs(
                    experiment_ids=exp_ids,
                    filter_string=filter_string,
                    order_by=["attributes.end_time DESC"],
                    max_results=50,
                )
            except Exception as se:  # noqa: BLE001
                print({"service": "inference", "event": "fallback_search_fail", "filter": filter_string, "error": str(se)})
                return []

        # Phase 1: explicitly prefer promoted=true tagged runs
        promoted_runs = _search("tags.promoted = 'true' and attributes.status = 'FINISHED'")
        chosen = None
        if promoted_runs:
            chosen = promoted_runs[0]
            try:
                pr_params = chosen.data.params or {}
                pr_cfg_hash = pr_params.get("config_hash") or pr_params.get("CONFIG_HASH")
                print({"service": "inference", "event": "fallback_model_load_promoted", "run_id": chosen.info.run_id, "model_type": pr_params.get("model_type"), "config_hash": pr_cfg_hash})
            except Exception:
                pass
        # Phase 2: fallback to any finished run if no promoted present
        if chosen is None:
            any_runs = _search("attributes.status = 'FINISHED'")
            if not any_runs:
                print({"service": "inference", "event": "fallback_no_runs_found"})
                return False
            chosen = any_runs[0]

        # 'chosen' now references the run to attempt loading
        run_id = chosen.info.run_id
        params = chosen.data.params or {}
        model_type = params.get("model_type") or params.get("MODEL_TYPE") or "model"
        config_hash = params.get("config_hash") or params.get("CONFIG_HASH")
        model_uri_candidates = [f"runs:/{run_id}/{model_type}"]
        if model_type != "model":  # add generic fallback path
            model_uri_candidates.append(f"runs:/{run_id}/model")
        loaded = False
        for cand in model_uri_candidates:
            try:
                print({"service": "inference", "event": "fallback_model_load_attempt", "model_uri": cand, "run_id": run_id})
                mdl = pyfunc.load_model(cand)
                _inf.current_model = mdl
                _inf.current_run_id = run_id
                _inf.current_run_name = model_type
                _inf.model_type = model_type
                _inf.current_config_hash = config_hash
                # enrich (sequence lengths, class, etc.)
                try:
                    _enrich_loaded_model(_inf, run_id, model_type)
                except Exception as enrich_err:  # noqa: BLE001
                    print({"service": "inference", "event": "fallback_enrich_fail", "error": str(enrich_err)})
                print({"service": "inference", "event": "startup_model_fallback_loaded", "run_id": run_id, "model_type": model_type, "config_hash": config_hash})
                loaded = True
                break
            except Exception as load_err:  # noqa: BLE001
                print({"service": "inference", "event": "fallback_model_load_fail", "candidate": cand, "error": str(load_err)})
        return loaded
    except Exception as e:  # noqa: BLE001
        print({"service": "inference", "event": "fallback_unhandled_error", "error": str(e)})
        return False

class PredictRequest(BaseModel):
    inference_length: Optional[int] = Field(None, ge=1, le=10000)
    data: Optional[Dict[str, List[Any]]] = None
    index_col: Optional[str] = None
    if hasattr(BaseModel, "model_config"):
        model_config = {"extra": "allow"}

class _InferenceJob:
    __slots__ = ("req_id", "req_model", "inference_length_q", "future", "enqueue_time")
    def __init__(self, req_id: str, req_model: PredictRequest | None, inference_length_q: int | None):
        self.req_id = req_id
        self.req_model = req_model
        self.inference_length_q = inference_length_q
        loop = asyncio.get_event_loop()
        self.future: asyncio.Future = loop.create_future()
        self.enqueue_time = time.time()

def _queue_log(event: str, **extra):  # central helper for structured logs
    try:
        payload = {"service": "inference", "event": event}
        payload.update(extra)
        print(payload, flush=True)
    except Exception:
        pass

async def _start_workers_once():
    global _workers_started, _inference_queue
    if _workers_started:
        return
    _inference_queue = asyncio.Queue(maxsize=QUEUE_MAXSIZE)
    for i in range(QUEUE_WORKERS):
        asyncio.create_task(_worker_loop(i))
        # explicit per-worker start log (human readable + structured)
        print(f"[queue] worker-{i} started", flush=True)
        _queue_log("queue_worker_started", worker=i)
    _workers_started = True
    _queue_log("queue_workers_started", workers=QUEUE_WORKERS, maxsize=QUEUE_MAXSIZE, timeout=INFERENCE_TIMEOUT)

async def _worker_loop(worker_idx: int):  # pragma: no cover (long-lived)
    while True:
        job: _InferenceJob = await _inference_queue.get()  # type: ignore
        queue_metrics["active"] += 1
        wait_ms = int((time.time() - job.enqueue_time) * 1000)
        queue_metrics["total_wait_ms"] += wait_ms
        queue_metrics["wait_samples"] += 1
        _queue_log("queue_job_start", req_id=job.req_id, worker=worker_idx, active=queue_metrics["active"], qsize=_inference_queue.qsize() if _inference_queue else None, waited_ms=wait_ms)
        try:
            result = await asyncio.wait_for(_execute_inference(job.req_model, job.inference_length_q, job.req_id), timeout=INFERENCE_TIMEOUT)
            if not job.future.done():
                job.future.set_result(result)
            queue_metrics["completed"] += 1
            _queue_log("queue_job_done", req_id=job.req_id, worker=worker_idx, active=queue_metrics["active"], completed=queue_metrics["completed"])
        except asyncio.TimeoutError:
            queue_metrics["timeouts"] += 1
            if not job.future.done():
                job.future.set_exception(HTTPException(status_code=504, detail="Inference timed out"))
            _queue_log("queue_job_timeout", req_id=job.req_id, worker=worker_idx, timeouts=queue_metrics["timeouts"])
        except HTTPException as he:
            if he.status_code >= 500:
                queue_metrics["error_500_total"] += 1
                queue_metrics["last_error_type"] = "HTTPException"
            if not job.future.done():
                job.future.set_exception(he)
            _queue_log("queue_job_http_error", req_id=job.req_id, status_code=he.status_code)
        except Exception as e:  # noqa: BLE001
            queue_metrics["error_500_total"] += 1
            queue_metrics["last_error_type"] = e.__class__.__name__
            if not job.future.done():
                job.future.set_exception(HTTPException(status_code=500, detail=f"Worker error: {e}"))
            _queue_log("queue_job_error", req_id=job.req_id, error=str(e))
        finally:
            queue_metrics["active"] -= 1
            try:
                _inference_queue.task_done()  # type: ignore
            except Exception:
                pass

async def _execute_inference(req: PredictRequest | None, inference_length_param: int | None, req_id: str):
    inf = _get_inferencer()
    # Attempt autoload if model not present
    if inf.current_model is None:
        try:  # pragma: no cover
            import importlib
            main_mod = importlib.import_module("main")  # type: ignore
            if hasattr(main_mod, "_attempt_load_promoted"):
                main_mod._attempt_load_promoted(inf)
        except Exception:  # noqa: BLE001
            pass
    if inf.current_model is None:
        raise HTTPException(status_code=503, detail="Model not loaded yet")

    # Data resolution
    if req and req.data:
        try:
            df_tmp = pd.DataFrame(req.data)
            if req.index_col and req.index_col in df_tmp.columns:
                df_tmp[req.index_col] = pd.to_datetime(df_tmp[req.index_col], errors="coerce")
                df_tmp = df_tmp.set_index(req.index_col).sort_index()
            elif df_tmp.columns[0].lower() in {"ts", "time", "timestamp"}:
                idx_col = df_tmp.columns[0]
                df_tmp[idx_col] = pd.to_datetime(df_tmp[idx_col], errors="coerce")
                df_tmp = df_tmp.set_index(idx_col).sort_index()
            else:
                raise ValueError("No recognizable index column; supply 'index_col'.")
            df = df_tmp
        except Exception as e:  # noqa: BLE001
            traceback.print_exc()
            raise HTTPException(status_code=400, detail=f"Failed to parse provided data: {e}")
    else:
        if inf.df is None:
            raise HTTPException(status_code=400, detail="No cached dataframe available and no data provided.")
        df = inf.df

    # Underlying busy guard (still allow cached) — primarily defensive if model sets busy flag internally
    if getattr(inf, 'busy', False):
        allow_cached = os.getenv("PREDICT_ALLOW_CACHED", "1").lower() in {"1","true","yes"}
        if allow_cached and hasattr(inf, 'last_prediction_response') and inf.last_prediction_response:
            cached = inf.last_prediction_response.copy()
            cached["status"] = "SUCCESS_CACHED"
            cached["cached"] = True
            cached["req_id"] = req_id
            queue_metrics["served_cached"] += 1
            _queue_log("predict_served_cached_busy", req_id=req_id, served_cached=queue_metrics["served_cached"])
            return cached
        queue_metrics["rejected_busy"] += 1
        _queue_log("predict_rejected_busy", req_id=req_id, rejected_busy=queue_metrics["rejected_busy"])
        raise HTTPException(status_code=429, detail="Inference busy, try again")

    eff_len = inference_length_param if inference_length_param is not None else (req.inference_length if req and req.inference_length is not None else 1)
    if os.getenv("PREDICT_STUB", "0") in {"1", "true", "TRUE"}:
        return {"status": "SUCCESS", "identifier": os.getenv("IDENTIFIER") or "default", "run_id": getattr(inf, "current_run_id", None), "predictions": []}

    try:
        # Wrap perform_inference in a thread function that enforces single-flight execution.
        def _locked_infer():
            with _MODEL_INFER_LOCK:
                return inf.perform_inference(df, inference_length=eff_len)
        preds = await asyncio.to_thread(_locked_infer)
        if preds is None:
            raise HTTPException(status_code=500, detail="Inference skipped (see server logs)")
        identifier = (os.getenv("IDENTIFIER") or "default") or "default"
        cols = [c for c in (["value"] if "value" in preds.columns else preds.columns.tolist())]
        pred_list = []
        for ts, row in preds[cols].iterrows():
            try:
                ts_serial = ts.isoformat()
            except Exception:
                ts_serial = str(ts)
            entry = {"ts": ts_serial}
            for c in cols:
                try:
                    val = row[c]
                    if val is None or (isinstance(val, (float, int)) and (not math.isfinite(float(val)))):
                        entry[c] = None
                    elif pd.isna(val):  # type: ignore[attr-defined]
                        entry[c] = None
                    else:
                        entry[c] = float(val)
                except Exception:
                    entry[c] = None
            pred_list.append(entry)
        resp = {
            "status": "SUCCESS",
            "identifier": identifier,
            "run_id": getattr(inf, "current_run_id", None),
            "predictions": pred_list,
        }
        try:
            inf.last_prediction_response = resp  # type: ignore[attr-defined]
        except Exception:
            pass
        return resp
    except HTTPException:
        raise
    except Exception as e:  # noqa: BLE001
        traceback.print_exc()
        if any(k in str(e).lower() for k in ["broken pipe", "connection reset", "client disconnected"]):
            raise HTTPException(status_code=499, detail="Client connection lost during response")
        raise HTTPException(status_code=500, detail=f"Inference error: {e}")

# --- Middleware to log ALL requests early (captures 422 JSON errors) ---
@app.middleware("http")
async def log_raw_request(request: Request, call_next):  # type: ignore
    try:
        print({
            "service": "inference",
            "event": "http_request_in",
            "method": request.method,
            "path": request.url.path,
        })
    except Exception:
        pass
    response = await call_next(request)
    return response

@app.get("/healthz")
def healthz():
    return {"status": "ok", "service": "inference-api"}

def _get_inferencer():
    from main import inferencer, _start_runtime  # type: ignore
    try:
        _start_runtime()
    except Exception:
        pass
    return inferencer

async def _prewarm_if_needed():  # pragma: no cover (performance side-effect)
    if not PREWARM_ENABLED:
        return
    try:
        inf = _get_inferencer()
        if getattr(inf, "current_model", None) is None:
            return
        # Skip if we already prewarmed this run id
        if hasattr(inf, "_last_prewarm_run_id") and getattr(inf, "_last_prewarm_run_id") == getattr(inf, "current_run_id", None):
            return
        t0 = time.time()
        # PyTorch models: run a minimal inference using existing df if available
        if getattr(inf, "model_class", "").lower() == "pytorch" and inf.df is not None:
            try:
                # Use perform_inference in a thread to avoid blocking loop
                await asyncio.to_thread(inf.perform_inference, inf.df, 1)
            except Exception as ie:  # noqa: BLE001
                _queue_log("prewarm_fail", error=str(ie))
        # Prophet / statsforecast usually compile lazily; trigger generic perform_inference
        elif getattr(inf, "model_class", "").lower() in {"prophet", "statsforecast"} and inf.df is not None:
            try:
                await asyncio.to_thread(inf.perform_inference, inf.df, 1)
            except Exception as ie:  # noqa: BLE001
                _queue_log("prewarm_fail", error=str(ie))
        setattr(inf, "last_prewarm_ms", int((time.time() - t0) * 1000))
        setattr(inf, "_last_prewarm_run_id", getattr(inf, "current_run_id", None))
        _queue_log("prewarm_complete", run_id=getattr(inf, "current_run_id", None), ms=getattr(inf, "last_prewarm_ms", None))
    except Exception as e:  # noqa: BLE001
        _queue_log("prewarm_wrapper_fail", error=str(e))

@app.get("/predict_ping")
def predict_ping():
    try:
        inf = _get_inferencer()
        return {
            "status": "ok",
            "model_loaded": inf.current_model is not None,
            "has_df": inf.df is not None,
            "busy": getattr(inf, "busy", False),
            "input_seq_len": getattr(inf, "input_seq_len", None),
            "output_seq_len": getattr(inf, "output_seq_len", None),
        }
    except Exception as e:  # noqa: BLE001
        return {"status": "error", "error": str(e)}

@app.get("/metrics")
def metrics():
    """Lightweight live metrics snapshot (<5ms target)."""
    inf = None
    try:
        inf = _get_inferencer()
    except Exception:
        pass
    build_version = os.getenv("INFER_VERSION")
    qsize = _inference_queue.qsize() if _inference_queue else 0
    avg_wait = None
    if queue_metrics["wait_samples"]:
        avg_wait = queue_metrics["total_wait_ms"] / max(1, queue_metrics["wait_samples"])
    return {
        "queue_length": qsize,
        "workers": QUEUE_WORKERS,
        "completed": queue_metrics["completed"],
        "rejected": queue_metrics["rejected_full"] + queue_metrics["rejected_busy"],
        "timeouts": queue_metrics["timeouts"],
        "error_500_total": queue_metrics["error_500_total"],
        "last_error_type": queue_metrics["last_error_type"],
        "model_loaded": bool(getattr(inf, "current_model", None)) if inf else False,
        "current_model_hash": getattr(inf, "current_config_hash", None) if inf else None,
        "current_run_id": getattr(inf, "current_run_id", None) if inf else None,
        "current_model_type": getattr(inf, "model_type", None) if inf else None,
        "startup_latency_ms": _startup_ready_ms,
        "prewarm_latency_ms": getattr(inf, "last_prewarm_ms", None) if inf else None,
        "average_queue_wait_ms": avg_wait,
        "served_cached": queue_metrics["served_cached"],
        "max_queue": QUEUE_MAXSIZE,
        "build_version": build_version,
        "status": "ok",
    }

@app.post("/reload_latest")
async def reload_latest():  # pragma: no cover (operational endpoint)
    """Manually trigger fallback load of newest MLflow run (without promotion manifest).

    Returns JSON describing outcome. Always returns 200 even on failure (status field indicates state).
    """
    loaded = await asyncio.to_thread(_fallback_load_latest_model, "manual")
    inf = None
    try:
        inf = _get_inferencer()
    except Exception:  # noqa: BLE001
        pass
    return {
        "status": "loaded" if loaded else "not_loaded",
        "run_id": getattr(inf, "current_run_id", None) if inf else None,
        "model_type": getattr(inf, "model_type", None) if inf else None,
        "config_hash": getattr(inf, "current_config_hash", None) if inf else None,
    }

@app.post("/predict")
async def predict(
    req: PredictRequest | None = Body(default={}),
    inference_length: int | None = Query(default=None, ge=1, le=10000),
):
    await _start_workers_once()
    req_id = uuid.uuid4().hex[:8]
    # Serve cached response instantly for empty {} request if available (no new job enqueued)
    try:
        if (req is None) or ( (not getattr(req, 'data', None)) and inference_length is None and getattr(req, 'inference_length', None) is None):
            inf_cached = _get_inferencer()
            if hasattr(inf_cached, 'last_prediction_response') and getattr(inf_cached, 'last_prediction_response'):
                cached = getattr(inf_cached, 'last_prediction_response').copy()
                cached["status"] = "SUCCESS_CACHED"
                cached["cached"] = True
                cached["req_id"] = req_id
                queue_metrics["served_cached"] += 1
                _queue_log("predict_served_cached_direct", req_id=req_id, served_cached=queue_metrics["served_cached"])
                return cached
    except Exception:
        # Silent failover to normal path
        pass
    if os.getenv("PREDICT_FORCE_OK", "0") in {"1", "true", "TRUE"}:
        _queue_log("predict_force_ok", req_id=req_id)
        return {"status": "SUCCESS", "identifier": os.getenv("IDENTIFIER") or "default", "run_id": None, "predictions": []}

    job = _InferenceJob(req_id=req_id, req_model=req, inference_length_q=inference_length)
    if _inference_queue is None:
        raise HTTPException(status_code=500, detail="Queue not initialized")
    try:
        _inference_queue.put_nowait(job)
        queue_metrics["enqueued"] += 1
        _queue_log("queue_job_enqueued", req_id=req_id, qsize=_inference_queue.qsize(), enqueued=queue_metrics["enqueued"], active=queue_metrics["active"])
    except asyncio.QueueFull:
        queue_metrics["rejected_full"] += 1
        _queue_log("queue_job_rejected_full", req_id=req_id, rejected=queue_metrics["rejected_full"], qsize=_inference_queue.qsize() if _inference_queue else None)
        # Provide Retry-After hint (100ms) to cooperative clients
        raise HTTPException(status_code=429, detail="Server busy, try again", headers={"Retry-After": "0.1"})

    try:
        return await job.future
    except HTTPException as he:  # Mask internal 5xx details with generic message
        if he.status_code >= 500:
            queue_metrics["error_500_total"] += 1
            queue_metrics["last_error_type"] = "HTTPException"
            _queue_log("predict_internal_error_masked", req_id=req_id, original_detail=str(he.detail))
            raise HTTPException(status_code=500, detail="Internal inference error")
        raise
    except Exception as e:  # noqa: BLE001
        queue_metrics["error_500_total"] += 1
        queue_metrics["last_error_type"] = e.__class__.__name__
        _queue_log("predict_unexpected_error", req_id=req_id, error=str(e))
        raise HTTPException(status_code=500, detail="Internal inference error")

@app.get("/queue_stats")
def queue_stats():
    qsize = _inference_queue.qsize() if _inference_queue else 0
    return {"status": "ok", "qsize": qsize, **queue_metrics, "workers": QUEUE_WORKERS, "maxsize": QUEUE_MAXSIZE}


# ---------------- Startup Readiness Gate (optional) -----------------
@app.on_event("startup")
async def _wait_for_model_startup():  # pragma: no cover (startup side-effect)
    global _startup_ready_ms
    # Ensure workers are ready early so that once model arrives we can serve instantly.
    await _start_workers_once()
    if not WAIT_FOR_MODEL:
        _startup_ready_ms = int((time.time() - _startup_epoch) * 1000)
        _queue_log("startup_no_wait_model", ready_ms=_startup_ready_ms)
        return
    deadline = time.time() + MODEL_WAIT_TIMEOUT
    logged_first = False
    while time.time() < deadline:
        inf = _get_inferencer()
        if inf.current_model is not None:
            _startup_ready_ms = int((time.time() - _startup_epoch) * 1000)
            _queue_log("startup_model_ready", ready_ms=_startup_ready_ms)
            # Fire prewarm (don't block readiness longer than needed, but await once)
            try:
                await _prewarm_if_needed()
            except Exception:
                pass
            return
        if not logged_first:
            _queue_log("startup_waiting_for_model", timeout_sec=MODEL_WAIT_TIMEOUT)
            logged_first = True
        await asyncio.sleep(1.0)
    # Timeout elapsed and still no model; attempt a last-chance fallback load
    try:
        loaded = await asyncio.to_thread(_fallback_load_latest_model, "timeout_fallback")
        if loaded:
            inf = _get_inferencer()
            _startup_ready_ms = int((time.time() - _startup_epoch) * 1000)
            _queue_log("startup_model_ready_fallback", ready_ms=_startup_ready_ms, run_id=getattr(inf, "current_run_id", None))
            try:
                await _prewarm_if_needed()
            except Exception:
                pass
            return
    except Exception:
        pass
    _startup_ready_ms = int((time.time() - _startup_epoch) * 1000)
    _queue_log("startup_model_wait_timeout", waited_ms=_startup_ready_ms)
    # If model appears shortly after timeout, a later request will trigger prewarm lazily.

